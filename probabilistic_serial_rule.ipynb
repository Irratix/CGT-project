{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import permutations\n",
    "from collections import Counter\n",
    "from copy import deepcopy\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generating Profiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate all possible partial preference profiles for one agent\n",
    "def generate_preferences(house_amt):\n",
    "    return list(permutations(range(house_amt)))\n",
    "\n",
    "# generate partial preference profile\n",
    "def generate_preference(house_amt):\n",
    "    return random.sample(range(house_amt), house_amt)\n",
    "\n",
    "# generate preference profile\n",
    "def generate_profile(agent_amt, house_amt):\n",
    "    profile = []\n",
    "    for agent in range(agent_amt):\n",
    "        profile.append(generate_preference(house_amt))\n",
    "    return profile\n",
    "\n",
    "# generates a hash from some profile, useful for preventing non-halting best-response learning\n",
    "def hash_profile(profile):\n",
    "    return hash(tuple(tuple(agent) for agent in profile))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Probabilistic Serial Rule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate probability matrix using the probabilistic serial rule\n",
    "def probability_matrix(profile):\n",
    "    agent_amt = len(profile)\n",
    "    house_amt = len(profile[0])\n",
    "    # init data of how much of each house is remaining and who ate how much\n",
    "    houses = [1] * house_amt\n",
    "    agents = [[0] * house_amt for _ in range(agent_amt)]\n",
    "\n",
    "    # probabilistic serial rule\n",
    "    while max(houses) != 0:\n",
    "        # for each player, find out which house they're eating\n",
    "        houses_selected = [next(i for i in preference if houses[i] != 0) for preference in profile]\n",
    "\n",
    "        # how long does it take to eat each house\n",
    "        house_counts = Counter(houses_selected)\n",
    "        total_rate_eaten = [house_counts.get(i, 0) for i in range(house_amt)]\n",
    "        time_to_eat = [None if total_rate_eaten[i] == 0 else houses[i] / total_rate_eaten[i] # Condition to avoid division by zero\n",
    "                 for i in range(house_amt)]\n",
    "\n",
    "        # take the lowest time to eat and step forward that amount of time\n",
    "        t = min(t for t in time_to_eat if t is not None) # poor second condition to avoid floating point errors\n",
    "        for agent in range(agent_amt):\n",
    "            house = houses_selected[agent]\n",
    "            houses[house] -= t\n",
    "            agents[agent][house] += t\n",
    "        houses = [house if house > 1e-9 else 0 for house in houses] # \n",
    "    return agents\n",
    "\n",
    "# calculate expected utility for one player from applying Borda scores of a profile to a probability matrix\n",
    "# Borda score starts at 0\n",
    "def expected_utility(matrix, profile, agent):\n",
    "    agent_amt = len(profile)\n",
    "    house_amt = len(profile[agent])\n",
    "    utility = 0\n",
    "    for house in range(house_amt):\n",
    "        house_value = house_amt - 1 - profile[agent].index(house)\n",
    "        utility += matrix[agent][house] * house_value\n",
    "    return utility\n",
    "\n",
    "# calculate expected utility for all players, see def expected_utility for detail\n",
    "def expected_utilities(matrix, profile):\n",
    "    agent_amt = len(profile)\n",
    "    return [expected_utility(matrix, profile, agent) for agent in range(agent_amt)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Best-Response Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Given a certain true profile, and a certain player, find a best response for that player and its utility\n",
    "def find_best_response(true_profile, profile, agent):\n",
    "    house_amt = len(profile[agent])\n",
    "    profile = deepcopy(profile)\n",
    "    # Prioritize the current action, if supplied\n",
    "    best_eu = -1\n",
    "    best_a = profile[agent]\n",
    "    if profile[agent] != None:\n",
    "        matrix = probability_matrix(profile)\n",
    "        best_eu = expected_utility(matrix, true_profile, agent)\n",
    "\n",
    "    # Loop over the agent's action space\n",
    "    for action in generate_preferences(house_amt):\n",
    "        profile[agent] = action\n",
    "        matrix = probability_matrix(profile)\n",
    "        eu = expected_utility(matrix, true_profile, agent)\n",
    "        if eu > best_eu:\n",
    "            best_eu = eu\n",
    "            best_a = action\n",
    "    return [list(best_a), best_eu]\n",
    "\n",
    "# Performs best-response learning, returning the PNE and the amount of steps to reach it, or None and the amount of steps to reach a loop\n",
    "def best_response_learning(true_profile, profile, visited=None, depth = 0):\n",
    "    # print(\"Current profile: \", profile)\n",
    "    if visited is None:\n",
    "        visited = []\n",
    "    # Loop detected, no PNE found here\n",
    "    h = hash_profile(profile)\n",
    "    if h in visited:\n",
    "        return [None, depth]\n",
    "    \n",
    "    visited.append(h)\n",
    "    agent_amt = len(profile)\n",
    "    house_amt = len(profile[0])\n",
    "    \n",
    "    matrix = probability_matrix(profile)\n",
    "    eu = expected_utilities(matrix, true_profile)\n",
    "    # print(\"Current EUs: \", eu)\n",
    "    for agent in range(agent_amt):\n",
    "        br, br_eu = find_best_response(true_profile, profile, agent)\n",
    "        if br_eu > eu[agent]:\n",
    "            # print(agent, br_eu, eu)\n",
    "            profile = deepcopy(profile)\n",
    "            profile[agent] = br\n",
    "            return best_response_learning(true_profile, profile, visited, depth + 1)\n",
    "    return [profile, depth]\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
